<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Driver Profile | Sai Surya Vidul</title>
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Chakra+Petch:wght@400;700&family=Syncopate:wght@700&display=swap" rel="stylesheet">
</head>
<body>
    <div id="lights-container">
        <div class="light"></div>
        <div class="light"></div>
        <div class="light"></div>
        <div class="light"></div>
        <div class="light"></div>
        <p class="status-text">WAIT FOR LIGHTS...</p>
    </div>

    <main id="main-content" class="hidden">
        <nav class="telemetry-nav">
            <div class="stat">DRV: <span>S. VIDUL CHINTHAMANENI</span></div>
            <div class="stat">TEAM: <span class="red">TASL @ UC RIVERSIDE</span></div>
            <div class="stat">CURRENT LAP TIME: <span id="clock">00:00:00</span></div>
        </nav>

        <section class="panel about-section">
            <div class="driver-card">
                <div class="driver-number">11</div>
                <div class="bio-text">
                    <h1 class="glitch" data-text="SAI SURYA VIDUL">SAI SURYA VIDUL CHINTHAMANENI</h1><p>
                        My race started in Hyderabad, earning a Bachelor’s in Electrical & Electronics Engineering at GRIET, before shifting gears to the international circuit at UC Riverside. Having graduated with a Master’s in Electrical Engineeringand a 3.82 GPA, I am LA based and focused on my core expertise in LLM/VLM fine-tuning, model inference, and evaluation. My research at the TASL Labinvolved architecting efficient reasoning frameworks for large-scale models to reduce computational latency. When I'm off the clock, you can find me watching F1, gaming, hitting the gym, or catching up on the latest anime. I leverage Python, PyTorch, and AWSto build AI solutions that are as high-performing as they are reliable.
                    </p>
                </div>
            </div>
        </section>

        <section class="panel history-full">
            <h2 class="panel-header">RACE HISTORY (WORK EXPERIENCE)</h2>
            
            <div class="history-item">
                <span class="year">2025-PRES</span>
                <strong>TASL - Graduate Student Researcher</strong><ul>
                    <li>Architected computation-efficient Embodied AI frameworks for Object Goal Navigation using Python and PyTorch.</li>
                    <li>Implemented sparse-inference reasoning triggers for Large Vision-Language Models (VLMs) to reduce computational latency.</li>
                    <li>Engineered hybrid two-stage training pipelines integrating Imitation Learning for initialization and Reinforcement Learning for fine-tuning.</li>
                </ul>
            </div>

            <div class="history-item">
                <span class="year">2022-2023</span>
                <strong>PathFinder Info Solutions - ML Intern</strong><ul>
                    <li>Engineered high-dimensional feature pipelines for matching engines, processing 25K+ user profiles via SQL and Pandas.</li>
                    <li>Boosted recommendation precision by 17% over heuristic baselines using scikit-learn and GridSearchCV.</li>
                    <li>Deployed end-to-end Streamlit dashboards on AWS to validate sentiment-adjusted outputs and ranking logic.</li>
                </ul>
            </div>

            <div class="history-item">
                <span class="year">2022</span>
                <strong>Gokul - Machine Learning Intern</strong><ul>
                    <li>Spearheaded time-series modeling for embedded water control systems to enable fault detection and predictive maintenance.</li>
                    <li>Partnered with R&D in C to interpret control board outputs and optimize hardware-software sensor integration.</li>
                </ul>
            </div>
        </section>

        <div class="content-grid">
            <section class="panel education">
                <h2 class="panel-header">ACADEMIC BACKGROUND</h2>
                <div class="history-item">
                    <span class="year">2023-2025</span>
                    <strong>University of California - Riverside</strong><p>M.S. in Electrical Engineering</p>
                    <p class="red">GPA: 3.82</p>
                </div>
                <div class="history-item">
                    <span class="year">2019-2023</span>
                    <strong>Jawaharlal Nehru Technological University - GRIET</strong><p>B.S. in Electrical and Electronics Engineering</p>
                    <p class="red">GPA: 3.39</p>
                </div>
            </section>

            <section class="panel stats">
                <h2 class="panel-header">TECHNICAL SPECS (PERFORMANCE DATA)</h2>
                <div class="stat-group">
                    <h4>CORE EXPERTISE</h4>
                    <p>LLM/VLM Fine-tuning (LoRA, RLHF), Model Inference, Evaluation, Computer Vision</p>
                </div>
                <div class="stat-group">
                    <h4>ENGINE / LANGUAGES</h4>
                    <p>Python, SQL, C</p>
                </div>
                <div class="stat-group">
                    <h4>CHASSIS / TOOLS</h4>
                    <p>PyTorch, AWS, Docker, Hugging Face, Weights & Biases, OpenCV</p>
                </div>
            </section>
        </div>

        <section class="panel projects-full">
            <h2 class="panel-header">SEASON HIGHLIGHTS (KEY PROJECTS)</h2>
            <div class="project-grid">
                <div class="project-card">
                    <span class="sector-tag">SAFETY_ALIGNMENT</span>
                    <h3>LLaVA-1.5 Safety RLHF</h3><p>Mitigated layer-wise vulnerabilities in LLaVA-1.5 using RLHF and PPO on AdvBench. Applied LoRA fine-tuning on Vicuna 7B with PyTorch multi-GPU parallelism.</p>
                </div>
                <div class="project-card">
                    <span class="sector-tag">VISUAL_REASONING</span>
                    <h3>CLIP + ViT Dual-Encoder</h3><p>Architected a VQA framework in PyTorch by integrating CLIP and Vision Transformers (ViT). Achieved 46.07% accuracy on a 50K VQA v2 sample subset.</p>
                </div>
            </div>
        </section>
    </main>

    <script src="script.js"></script>
</body>
</html>